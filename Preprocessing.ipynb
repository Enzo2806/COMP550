{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/enzobenoit-\n",
      "[nltk_data]     jeannin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json # for loading the json file\n",
    "from torch.utils.data import Dataset # for creating the dataset\n",
    "\n",
    "# for tokenizing the sentences\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('punkt') \n",
    "\n",
    "# For splitting the dataset into train, validation anmd testing\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from TranslationDataset import TranslationDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the header json file were we store paths / parameters / etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON header file\n",
    "def load_json_header(json_file):\n",
    "    with open(json_file) as json_data:\n",
    "        d = json.load(json_data)\n",
    "        return d\n",
    "\n",
    "config = load_json_header('config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the set of functions we will used to preprocess the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to load the data from the files. The file paths are defined in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(source_file_path, target_file_path):\n",
    "    \"\"\"\n",
    "    Load data from two separate files where each line in one file corresponds to the line in the other file.\n",
    "    \"\"\"\n",
    "    with open(source_file_path, 'r', encoding='utf-8') as file:\n",
    "        source = file.read().split('\\n')\n",
    "    with open(target_file_path, 'r', encoding='utf-8') as file:\n",
    "        target = file.read().split('\\n')\n",
    "\n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, min_frequency=2, special_tokens=[\"[UNK]\", \"[SOS]\", \"[EOS]\", \"[PAD]\"]):\n",
    "    vocab = {}\n",
    "    word_counts = {}\n",
    "\n",
    "    # Initialize vocab with special tokens\n",
    "    for token in special_tokens:\n",
    "        vocab[token] = len(vocab)\n",
    "\n",
    "    # Count word frequencies\n",
    "    for sentence in sentences:\n",
    "        for word in word_tokenize(sentence):\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "    # Print some examples from the imported dataset\n",
    "    print(\"Some tokenized examples from the imported dataset:\")\n",
    "    for i in range(5):\n",
    "        print(word_tokenize(sentences[i]))\n",
    "        \n",
    "    # Add words above min frequency to vocab\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_frequency:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split(source_sentences, target_sentences, test_size, val_size):\n",
    "    combined = list(zip(source_sentences, target_sentences))\n",
    "    random.shuffle(combined)\n",
    "    shuffled_source_sentences, shuffled_target_sentences = zip(*combined)\n",
    "\n",
    "    # Splitting into train and test\n",
    "    src_train_val, src_test, trg_train_val, trg_test = train_test_split(\n",
    "        shuffled_source_sentences, shuffled_target_sentences, test_size = test_size, random_state =42)\n",
    "    \n",
    "    # Splitting train_val into train and val\n",
    "    src_train, src_val, trg_train, trg_val = train_test_split(\n",
    "        src_train_val, trg_train_val, test_size = val_size/(1 - test_size), random_state=42)\n",
    "\n",
    "    return src_train, src_val, src_test, trg_train, trg_val, trg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datasets(source_file_path, target_file_path, save_path):\n",
    "    \"\"\"\n",
    "    Preprocess the datasets and save them to files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data from source and target files\n",
    "    source, target = load_data(source_file_path, target_file_path)\n",
    "\n",
    "    # Check if source and target files have the same number of lines\n",
    "    if len(source) != len(target):\n",
    "        raise Exception(\"Source and target files do not have the same number of examples.\")\n",
    "\n",
    "    # Build vocab dictionaries for source and target languages\n",
    "    source_vocab = build_vocab(source)\n",
    "    target_vocab = build_vocab(target)\n",
    "\n",
    "    # We first get the validation and testing set sizes located in the config file\n",
    "    # We then calculate the training set size from them and the total dataset size (see the shuffle and split function above)\n",
    "    val_size = config[\"val_size\"]\n",
    "    test_size = config[\"test_size\"]\n",
    "\n",
    "    # Split into train and validation sets, making sure that the source and target sentences are aligned\n",
    "    src_train, src_val, src_test, trg_train, trg_val, trg_test = shuffle_and_split(source, target, test_size=test_size, val_size=val_size)\n",
    "\n",
    "    # Check that the source and target datasets have the same size after split, otherwise raise an exception\n",
    "    if len(src_train) != len(trg_train) or len(src_val) != len(trg_val) or len(src_test) != len(trg_test):\n",
    "        raise Exception(\"Source and target datasets do not have the same size\")\n",
    "    \n",
    "    # Get the maximum sequence length from the config file\n",
    "    max_seq_len = config[\"max_seq_len\"]\n",
    "    \n",
    "    # Create datasets\n",
    "    # We use the TranslationDataset class defined above\n",
    "    # We pass the source and target datasets, source and target languages, source and target vocabularies, and the sequence length (in the config file)\n",
    "    train_ds = TranslationDataset(src_train, trg_train, source_vocab, target_vocab, max_seq_len)\n",
    "    val_ds = TranslationDataset(src_val, trg_val, source_vocab, target_vocab, max_seq_len)\n",
    "    test_ds = TranslationDataset(src_test, trg_test, source_vocab, target_vocab, max_seq_len)\n",
    "\n",
    "    # Print some examples from the dataset    print(\"Some examples from the dataset:\")\n",
    "    for i in range(1):\n",
    "        print(train_ds[i])\n",
    "        \n",
    "    # Print dataset sizes and a sample from the training dataset\n",
    "    print(\"Train dataset size:\", len(train_ds))\n",
    "    print(\"Validation dataset size:\", len(val_ds))\n",
    "    print(\"Test dataset size:\", len(test_ds))\n",
    "\n",
    "    # Save the datasets to files\n",
    "    torch.save(train_ds, save_path + \"train_ds.pt\")\n",
    "    torch.save(val_ds, save_path + \"val_ds.pt\")\n",
    "    torch.save(test_ds, save_path + \"test_ds.pt\")\n",
    "\n",
    "    # Save the vocabularies to files\n",
    "    torch.save(source_vocab, save_path + \"source_vocab.pt\")\n",
    "    torch.save(target_vocab, save_path + \"target_vocab.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing english to italian dataset:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess the datasets\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing english to italian dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpreprocess_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men-it-dataset-english\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men-it-dataset-italian\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men-it-save-path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing english to spanish dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m, in \u001b[0;36mpreprocess_datasets\u001b[0;34m(source_file_path, target_file_path, save_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource and target files do not have the same number of examples.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Build vocab dictionaries for source and target languages\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m source_vocab \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m target_vocab \u001b[38;5;241m=\u001b[39m build_vocab(target)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# We first get the validation and testing set sizes located in the config file\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# We then calculate the training set size from them and the total dataset size (see the shuffle and split function above)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m, in \u001b[0;36mbuild_vocab\u001b[0;34m(sentences, min_frequency, special_tokens)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Count word frequencies\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     12\u001b[0m         word_counts[word] \u001b[38;5;241m=\u001b[39m word_counts\u001b[38;5;241m.\u001b[39mget(word, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Print some examples from the imported dataset\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/COMP550/myenv/lib/python3.9/site-packages/nltk/tokenize/__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/GitHub/COMP550/myenv/lib/python3.9/site-packages/nltk/tokenize/__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/GitHub/COMP550/myenv/lib/python3.9/site-packages/nltk/tokenize/destructive.py:181\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    178\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[0;32m--> 181\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m1 \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m2 \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS3:\n\u001b[1;32m    183\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Preprocess the datasets\n",
    "print(\"Preprocessing english to italian dataset:\")\n",
    "preprocess_datasets(config['en-it-dataset-english'] , config['en-it-dataset-italian'], config['en-it-save-path'])\n",
    "\n",
    "print()\n",
    "print(\"Preprocessing english to spanish dataset:\")\n",
    "preprocess_datasets(config['en-es-dataset-english'], config['en-es-dataset-spanish'], config['en-es-save-path'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
