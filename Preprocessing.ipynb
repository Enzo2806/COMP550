{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the set of imports that we will use to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/enzobenoit-\n",
      "[nltk_data]     jeannin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/enzobenoit-\n",
      "[nltk_data]     jeannin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json # for loading the json file\n",
    "\n",
    "# for tokenizing the sentences\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('punkt') \n",
    "\n",
    "# For splitting the dataset into train, validation anmd testing\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For creating the dataset class (see the TranslationDataset.py file)\n",
    "from TranslationDataset import TranslationDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the header json file were we store paths / parameters / etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON header file\n",
    "def load_json_header(json_file):\n",
    "    with open(json_file) as json_data:\n",
    "        d = json.load(json_data)\n",
    "        return d\n",
    "\n",
    "header_file = load_json_header('header.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the set of functions we will used to preprocess the data. This includes:\n",
    "- `load_data`: load both the source and target parts of the data\n",
    "- `build_vocab`: build the vocabulary from the imported source and target data\n",
    "- `shuffle_and_split`: shuffle and split the data into train, validation and test sets\n",
    "- `preprocess_datasets`: preprocess the data by applying the above functions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to load both the source and target parts of the data from files. The file paths are defined in the header json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(source_file_path, target_file_path):\n",
    "    \"\"\"\n",
    "    Load data from two separate files each corresponding to the source and target language.\n",
    "    Each line in one file corresponds to the translation in the other file. \n",
    "    The function returns two lists, one for the source and one for the target language.\n",
    "    \"\"\"\n",
    "    with open(source_file_path, 'r', encoding='utf-8') as file:\n",
    "        source = file.read().split('\\n')\n",
    "    with open(target_file_path, 'r', encoding='utf-8') as file:\n",
    "        target = file.read().split('\\n')\n",
    "\n",
    "    # We only keep a certain number of sentences for memory purposes\n",
    "    # The dataset we use has around 1.5 million sentences for each language\n",
    "    # The maximum number of sentences is defined in the header file\n",
    "    source = source[0: min(header_file[\"max_dataset_size\"], len(source))]\n",
    "    target = target[0: min(header_file[\"max_dataset_size\"], len(target))]\n",
    "\n",
    "    # Set to lowercase all the sentences in both languages \n",
    "    source = [sentence.lower() for sentence in source]\n",
    "    target = [sentence.lower() for sentence in target]\n",
    "\n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the function to build the vocabulary from the imported source and target data. We use the nltk tokenizer to tokenize the data and then build the vocabulary from the tokenized data. We also add the special tokens `<pad>`, `<unk>`, `<sos>` and `<eos>` to the vocabulary. These tokens are used to pad the sequences, replace unknown tokens and mark the start and end of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, min_frequency=2, special_tokens=[\"[UNK]\", \"[SOS]\", \"[EOS]\", \"[PAD]\"]):\n",
    "    \"\"\"\n",
    "    Build the vocabulary from the given sentences.\n",
    "    We use the nltk tokenizer to tokenize the data and then build the vocabulary from the tokenized data.\n",
    "    We also add the special tokens `<pad>`, `<unk>`, `<sos>` and `<eos>` to the vocabulary. \n",
    "    We only add words that appear at least `min_frequency` times in the dataset (default value is 2).\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    word_counts = {}\n",
    "\n",
    "    # Initialize vocab with special tokens\n",
    "    for token in special_tokens:\n",
    "        vocab[token] = len(vocab)\n",
    "\n",
    "    # Count word frequencies\n",
    "    for sentence in sentences:\n",
    "        for word in word_tokenize(sentence):\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "    # Print some examples from the imported dataset\n",
    "    print(\"Some tokenized examples:\")\n",
    "    for i in range(5):\n",
    "        print(word_tokenize(sentences[i]))\n",
    "        \n",
    "    # Add words above min frequency to vocab\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_frequency:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the function to shuffle and split the data into train, validation and test sets. We use the `train_test_split` function from sklearn to split the data into validation and test sets. We then split the validation set into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split(source_sentences, target_sentences, test_size, val_size):\n",
    "    \"\"\"\n",
    "    Shuffle the dataset and split it into train, validation and test sets.\n",
    "    \"\"\"\n",
    "    # Shuffle the datasets in the same order (so that the translation is still correct)\n",
    "    combined = list(zip(source_sentences, target_sentences))\n",
    "    random.shuffle(combined)\n",
    "    shuffled_source_sentences, shuffled_target_sentences = zip(*combined)\n",
    "\n",
    "    # Splitting into validation and test using the test_size parameter\n",
    "    src_train_val, src_test, trg_train_val, trg_test = train_test_split(\n",
    "        shuffled_source_sentences, shuffled_target_sentences, test_size = test_size, random_state =42)\n",
    "    \n",
    "    # Splitting the validation sets into train and validation using the val_size parameter \n",
    "    src_train, src_val, trg_train, trg_val = train_test_split(\n",
    "        src_train_val, trg_train_val, test_size = val_size/(1 - test_size), random_state=42)\n",
    "\n",
    "    return src_train, src_val, src_test, trg_train, trg_val, trg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally define the function to preprocess the data by applying the above functions. We save the constructed vocabularies and datasets to the corresponding files defined in the header json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens(token_ids, vocab):\n",
    "    \"\"\"\n",
    "    Decode a list of token IDs back to a sentence using the vocabulary.\n",
    "\n",
    "    Args:\n",
    "        token_ids (list of int): A list of token IDs.\n",
    "        vocab (dict): A dictionary mapping words to their token IDs.\n",
    "    \n",
    "    Returns:\n",
    "        sentence (str): The decoded sentence.\n",
    "    \"\"\"\n",
    "    # Create a reverse vocabulary\n",
    "    reverse_vocab = {id: word for word, id in vocab.items()}\n",
    "\n",
    "    # Decode the token IDs to words\n",
    "    words = [reverse_vocab.get(id, \"[UNK]\") for id in token_ids]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datasets(source_file_path, target_file_path, save_path):\n",
    "    \"\"\"\n",
    "    Preprocess the datasets and save them to their corresponding files.\n",
    "    \"\"\"\n",
    "    # Load data from source and target files\n",
    "    source, target = load_data(source_file_path, target_file_path)\n",
    "\n",
    "    # Check if source and target files have the same number of lines\n",
    "    if len(source) != len(target):\n",
    "        raise Exception(\"Source and target files do not have the same number of examples.\")\n",
    "\n",
    "    # Build vocab dictionaries for source and target languages\n",
    "    source_vocab = build_vocab(source)\n",
    "    target_vocab = build_vocab(target)\n",
    "\n",
    "    # We first get the validation and testing set sizes located in the config file\n",
    "    # We then calculate the training set size from them and the total dataset size (see the shuffle and split function above)\n",
    "    val_size = header_file[\"val_size\"]\n",
    "    test_size = header_file[\"test_size\"]\n",
    "\n",
    "    # Split into train and validation sets, making sure that the source and target sentences are aligned\n",
    "    src_train, src_val, src_test, trg_train, trg_val, trg_test = shuffle_and_split(source, target, test_size=test_size, val_size=val_size)\n",
    "\n",
    "    # Check that the source and target datasets have the same size after split, otherwise raise an exception\n",
    "    if len(src_train) != len(trg_train) or len(src_val) != len(trg_val) or len(src_test) != len(trg_test):\n",
    "        raise Exception(\"Source and target datasets do not have the same size\")\n",
    "    \n",
    "    # Get the maximum sequence length from the config file\n",
    "    max_seq_len = header_file[\"max_seq_len\"]\n",
    "    \n",
    "    # Create datasets\n",
    "    # We use the TranslationDataset class defined in the TranslationDataset.py file\n",
    "    # We pass the source and target datasets, source and target languages, source and target vocabularies, and the sequence length (in the header file)\n",
    "    train_ds = TranslationDataset(src_train, trg_train, source_vocab, target_vocab, max_seq_len)\n",
    "    val_ds = TranslationDataset(src_val, trg_val, source_vocab, target_vocab, max_seq_len)\n",
    "    test_ds = TranslationDataset(src_test, trg_test, source_vocab, target_vocab, max_seq_len)\n",
    "\n",
    "    # Print an example from the dataset \n",
    "    print(\"One example from the constructed dataset:\")\n",
    "    for i in range(1):\n",
    "        print(train_ds[i])\n",
    "        \n",
    "    # Print dataset sizes as a sanity check\n",
    "    print(\"Maximum dataset size:\", header_file[\"max_dataset_size\"])\n",
    "    print(\"Train dataset size:\", len(train_ds))\n",
    "    print(\"Validation dataset size:\", len(val_ds))\n",
    "    print(\"Test dataset size:\", len(test_ds))\n",
    "\n",
    "    # Save the datasets to files\n",
    "    torch.save(train_ds, save_path + \"train_ds.pt\")\n",
    "    torch.save(val_ds, save_path + \"val_ds.pt\")\n",
    "    torch.save(test_ds, save_path + \"test_ds.pt\")\n",
    "\n",
    "    # Save the vocabularies to files\n",
    "    torch.save(source_vocab, save_path + \"source_vocab.pt\")\n",
    "    torch.save(target_vocab, save_path + \"target_vocab.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing english to italian dataset:\n",
      "Some tokenized examples:\n",
      "['resumption', 'of', 'the', 'session']\n",
      "['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999', ',', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period', '.']\n",
      "['although', ',', 'as', 'you', 'will', 'have', 'seen', ',', 'the', 'dreaded', \"'millennium\", 'bug', \"'\", 'failed', 'to', 'materialise', ',', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful', '.']\n",
      "['you', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days', ',', 'during', 'this', 'part-session', '.']\n",
      "['in', 'the', 'meantime', ',', 'i', 'should', 'like', 'to', 'observe', 'a', 'minute', \"'\", 's', 'silence', ',', 'as', 'a', 'number', 'of', 'members', 'have', 'requested', ',', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned', ',', 'particularly', 'those', 'of', 'the', 'terrible', 'storms', ',', 'in', 'the', 'various', 'countries', 'of', 'the', 'european', 'union', '.']\n",
      "Some tokenized examples:\n",
      "['ripresa', 'della', 'sessione']\n",
      "['dichiaro', 'ripresa', 'la', 'sessione', 'del', 'parlamento', 'europeo', ',', 'interrotta', 'venerdì', '17', 'dicembre', 'e', 'rinnovo', 'a', 'tutti', 'i', 'miei', 'migliori', 'auguri', 'nella', 'speranza', 'che', 'abbiate', 'trascorso', 'delle', 'buone', 'vacanze', '.']\n",
      "['come', 'avrete', 'avuto', 'modo', 'di', 'constatare', 'il', 'grande', '``', 'baco', 'del', 'millennio', \"''\", 'non', 'si', 'è', 'materializzato', '.', 'invece', ',', 'i', 'cittadini', 'di', 'alcuni', 'nostri', 'paesi', 'sono', 'stati', 'colpiti', 'da', 'catastrofi', 'naturali', 'di', 'proporzioni', 'davvero', 'terribili', '.']\n",
      "['avete', 'chiesto', 'che', 'si', 'tenesse', 'una', 'discussione', 'su', 'tale', 'tema', 'nei', 'prossimi', 'giorni', ',', 'nel', 'corso', 'della', 'presente', 'tornata', '.']\n",
      "['nel', 'frattempo', 'è', 'mio', 'desiderio', ',', 'come', 'del', 'resto', 'mi', 'è', 'stato', 'chiesto', 'da', 'alcuni', 'colleghi', ',', 'osservare', 'un', 'minuto', 'di', 'silenzio', 'in', 'memoria', 'di', 'tutte', 'le', 'vittime', 'delle', 'tempeste', 'che', 'si', 'sono', 'abbattute', 'sui', 'diversi', 'paesi', 'dell', \"'\", 'unione', 'europea', '.']\n",
      "One example from the constructed dataset:\n",
      "{'encoder_input': tensor([   1,  122,  202,   32,   61, 1715,   34,  329,  751,  409,   89, 1041,\n",
      "          19,  158, 2950,    6,    0,  714,    5,   28,  315, 2951,    5,  833,\n",
      "        2060,   19, 2952, 2517,    6,  607,   39,    2,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3]), 'decoder_input': tensor([   1,   17,   99,   89,  236,  463,   27,    8,  869,  635, 1868, 3896,\n",
      "           8, 3897,  634,   38,   86, 3898,  349, 1123, 2115,   38,   83,   12,\n",
      "        3899, 1115,  749,   33,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3]), 'encoder_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.int32), 'decoder_mask': tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]]]), 'label': tensor([  17,   99,   89,  236,  463,   27,    8,  869,  635, 1868, 3896,    8,\n",
      "        3897,  634,   38,   86, 3898,  349, 1123, 2115,   38,   83,   12, 3899,\n",
      "        1115,  749,   33,    2,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3]), 'src': 'it is in this spirit that our parliamentary committee for example, has championed the longstanding idea of a public register of state aid, accessible via the internet.', 'trg': \"e' in questo spirito che la nostra commissione parlamentare difende la vecchia idea di un registro pubblico degli aiuti di stato, accessibile via internet.\"}\n",
      "Maximum dataset size: 20000\n",
      "Train dataset size: 15999\n",
      "Validation dataset size: 2001\n",
      "Test dataset size: 2000\n",
      "\n",
      "Preprocessing english to spanish dataset:\n",
      "Some tokenized examples:\n",
      "['resumption', 'of', 'the', 'session']\n",
      "['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999', ',', 'and', 'i', 'would', 'like', 'on', 'ce', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period', '.']\n",
      "['although', ',', 'as', 'you', 'will', 'have', 'seen', ',', 'the', 'dreaded', \"'millennium\", 'bug', \"'\", 'failed', 'to', 'materialise', ',', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful', '.']\n",
      "['you', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days', ',', 'during', 'this', 'part-session', '.']\n",
      "['in', 'the', 'meantime', ',', 'i', 'should', 'like', 'to', 'observe', 'a', 'minute', \"'\", 's', 'silence', ',', 'as', 'a', 'number', 'of', 'members', 'have', 'requested', ',', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned', ',', 'particularly', 'those', 'of', 'the', 'terrible', 'storms', ',', 'in', 'the', 'various', 'countries', 'of', 'the', 'european', 'union', '.']\n",
      "Some tokenized examples:\n",
      "['reanudación', 'del', 'período', 'de', 'sesiones']\n",
      "['declaro', 'reanudado', 'el', 'período', 'de', 'sesiones', 'del', 'parlamento', 'europeo', ',', 'interrumpido', 'el', 'viernes', '17', 'de', 'diciembre', 'pasado', ',', 'y', 'reitero', 'a', 'sus', 'señorías', 'mi', 'deseo', 'de', 'que', 'hayan', 'tenido', 'unas', 'buenas', 'vacaciones', '.']\n",
      "['como', 'todos', 'han', 'podido', 'comprobar', ',', 'el', 'gran', '``', 'efecto', 'del', 'año', '2000', \"''\", 'no', 'se', 'ha', 'producido', '.', 'en', 'cambio', ',', 'los', 'ciudadanos', 'de', 'varios', 'de', 'nuestros', 'países', 'han', 'sido', 'víctimas', 'de', 'catástrofes', 'naturales', 'verdaderamente', 'terribles', '.']\n",
      "['sus', 'señorías', 'han', 'solicitado', 'un', 'debate', 'sobre', 'el', 'tema', 'para', 'los', 'próximos', 'días', ',', 'en', 'el', 'curso', 'de', 'este', 'período', 'de', 'sesiones', '.']\n",
      "['a', 'la', 'espera', 'de', 'que', 'se', 'produzca', ',', 'de', 'acuerdo', 'con', 'muchos', 'colegas', 'que', 'me', 'lo', 'han', 'pedido', ',', 'pido', 'que', 'hagamos', 'un', 'minuto', 'de', 'silencio', 'en', 'memoria', 'de', 'todas', 'las', 'víctimas', 'de', 'las', 'tormentas', ',', 'en', 'los', 'distintos', 'países', 'de', 'la', 'unión', 'europea', 'afectados', '.']\n",
      "One example from the constructed dataset:\n",
      "{'encoder_input': tensor([   1,    8,  432,   99,   33,   76,    6, 2735, 3955, 3583, 3822,   31,\n",
      "        7458, 1158, 2148,   38,    2,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3]), 'decoder_input': tensor([    1,  1317,  1529,     7,    27,    49,    35,    51,  3285, 13109,\n",
      "           46,  6480,  1688,    67,  4602,  2520,  2807,    33,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3]), 'encoder_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.int32), 'decoder_mask': tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]]]), 'label': tensor([ 1317,  1529,     7,    27,    49,    35,    51,  3285, 13109,    46,\n",
      "         6480,  1688,    67,  4602,  2520,  2807,    33,     2,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3]), 'src': 'i am aware that all the studies launched experienced difficulty in collecting adequate data.', 'trg': 'soy consciente de que en todos los estudios emprendidos se tuvieron dificultades para recopilar datos suficientes.'}\n",
      "Maximum dataset size: 20000\n",
      "Train dataset size: 15999\n",
      "Validation dataset size: 2001\n",
      "Test dataset size: 2000\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the datasets\n",
    "print(\"Preprocessing english to italian dataset:\")\n",
    "preprocess_datasets(header_file['en-it-dataset-english'] , header_file['en-it-dataset-italian'], header_file['en-it-save-path'])\n",
    "\n",
    "print()\n",
    "print(\"Preprocessing english to spanish dataset:\")\n",
    "preprocess_datasets(header_file['en-es-dataset-english'], header_file['en-es-dataset-spanish'], header_file['en-es-save-path'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
