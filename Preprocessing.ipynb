{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/enzobenoit-\n",
      "[nltk_data]     jeannin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/enzobenoit-\n",
      "[nltk_data]     jeannin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json # for loading the json file\n",
    "from torch.utils.data import Dataset # for creating the dataset\n",
    "\n",
    "# for tokenizing the sentences\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('punkt') \n",
    "\n",
    "# For splitting the dataset into train, validation anmd testing\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from TranslationDataset import TranslationDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the header json file were we store paths / parameters / etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON header file\n",
    "def load_json_header(json_file):\n",
    "    with open(json_file) as json_data:\n",
    "        d = json.load(json_data)\n",
    "        return d\n",
    "\n",
    "config = load_json_header('config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the set of functions we will used to preprocess the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to load the data from the files. The file paths are defined in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(source_file_path, target_file_path):\n",
    "    \"\"\"\n",
    "    Load data from two separate files where each line in one file corresponds to the line in the other file.\n",
    "    \"\"\"\n",
    "    with open(source_file_path, 'r', encoding='utf-8') as file:\n",
    "        source = file.read().split('\\n')\n",
    "    with open(target_file_path, 'r', encoding='utf-8') as file:\n",
    "        target = file.read().split('\\n')\n",
    "\n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, min_frequency=2, special_tokens=[\"[UNK]\", \"[SOS]\", \"[EOS]\", \"[PAD]\"]):\n",
    "    vocab = {}\n",
    "    word_counts = {}\n",
    "\n",
    "    # Initialize vocab with special tokens\n",
    "    for token in special_tokens:\n",
    "        vocab[token] = len(vocab)\n",
    "\n",
    "    # Count word frequencies\n",
    "    for sentence in sentences:\n",
    "        for word in word_tokenize(sentence):\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "    # Print some examples from the imported dataset\n",
    "    print(\"Some tokenized examples from the imported dataset:\")\n",
    "    for i in range(5):\n",
    "        print(word_tokenize(sentences[i]))\n",
    "        \n",
    "    # Add words above min frequency to vocab\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_frequency:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split(source_sentences, target_sentences, test_size, val_size):\n",
    "    combined = list(zip(source_sentences, target_sentences))\n",
    "    random.shuffle(combined)\n",
    "    shuffled_source_sentences, shuffled_target_sentences = zip(*combined)\n",
    "\n",
    "    # Splitting into train and test\n",
    "    src_train_val, src_test, trg_train_val, trg_test = train_test_split(\n",
    "        shuffled_source_sentences, shuffled_target_sentences, test_size = test_size, random_state =42)\n",
    "    \n",
    "    # Splitting train_val into train and val\n",
    "    src_train, src_val, trg_train, trg_val = train_test_split(\n",
    "        src_train_val, trg_train_val, test_size = val_size/(1 - test_size), random_state=42)\n",
    "\n",
    "    return src_train, src_val, src_test, trg_train, trg_val, trg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datasets(source_file_path, target_file_path, save_path):\n",
    "    \"\"\"\n",
    "    Preprocess the datasets and save them to files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data from source and target files\n",
    "    source, target = load_data(source_file_path, target_file_path)\n",
    "\n",
    "    # Check if source and target files have the same number of lines\n",
    "    if len(source) != len(target):\n",
    "        raise Exception(\"Source and target files do not have the same number of examples.\")\n",
    "\n",
    "    # Build vocab dictionaries for source and target languages\n",
    "    source_vocab = build_vocab(source)\n",
    "    target_vocab = build_vocab(target)\n",
    "\n",
    "    # We first get the validation and testing set sizes located in the config file\n",
    "    # We then calculate the training set size from them and the total dataset size (see the shuffle and split function above)\n",
    "    val_size = config[\"val_size\"]\n",
    "    test_size = config[\"test_size\"]\n",
    "\n",
    "    # Split into train and validation sets, making sure that the source and target sentences are aligned\n",
    "    src_train, src_val, src_test, trg_train, trg_val, trg_test = shuffle_and_split(source, target, test_size=test_size, val_size=val_size)\n",
    "\n",
    "    # Check that the source and target datasets have the same size after split, otherwise raise an exception\n",
    "    if len(src_train) != len(trg_train) or len(src_val) != len(trg_val) or len(src_test) != len(trg_test):\n",
    "        raise Exception(\"Source and target datasets do not have the same size\")\n",
    "    \n",
    "    # Get the maximum sequence length from the config file\n",
    "    max_seq_len = config[\"max_seq_len\"]\n",
    "    \n",
    "    # Create datasets\n",
    "    # We use the TranslationDataset class defined above\n",
    "    # We pass the source and target datasets, source and target languages, source and target vocabularies, and the sequence length (in the config file)\n",
    "    train_ds = TranslationDataset(src_train, trg_train, source_vocab, target_vocab, max_seq_len)\n",
    "    val_ds = TranslationDataset(src_val, trg_val, source_vocab, target_vocab, max_seq_len)\n",
    "    test_ds = TranslationDataset(src_test, trg_test, source_vocab, target_vocab, max_seq_len)\n",
    "\n",
    "    # Print some examples from the dataset print(\"Some examples from the dataset:\")\n",
    "    for i in range(1):\n",
    "        print(train_ds[i])\n",
    "        \n",
    "    # Print dataset sizes and a sample from the training dataset\n",
    "    print(\"Train dataset size:\", len(train_ds))\n",
    "    print(\"Validation dataset size:\", len(val_ds))\n",
    "    print(\"Test dataset size:\", len(test_ds))\n",
    "\n",
    "    # Save the datasets to files\n",
    "    torch.save(train_ds, save_path + \"train_ds.pt\")\n",
    "    torch.save(val_ds, save_path + \"val_ds.pt\")\n",
    "    torch.save(test_ds, save_path + \"test_ds.pt\")\n",
    "\n",
    "    # Save the vocabularies to files\n",
    "    torch.save(source_vocab, save_path + \"source_vocab.pt\")\n",
    "    torch.save(target_vocab, save_path + \"target_vocab.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing english to italian dataset:\n",
      "Some tokenized examples from the imported dataset:\n",
      "['Resumption', 'of', 'the', 'session']\n",
      "['I', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'European', 'Parliament', 'adjourned', 'on', 'Friday', '17', 'December', '1999', ',', 'and', 'I', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period', '.']\n",
      "['Although', ',', 'as', 'you', 'will', 'have', 'seen', ',', 'the', 'dreaded', \"'millennium\", 'bug', \"'\", 'failed', 'to', 'materialise', ',', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful', '.']\n",
      "['You', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days', ',', 'during', 'this', 'part-session', '.']\n",
      "['In', 'the', 'meantime', ',', 'I', 'should', 'like', 'to', 'observe', 'a', 'minute', \"'\", 's', 'silence', ',', 'as', 'a', 'number', 'of', 'Members', 'have', 'requested', ',', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned', ',', 'particularly', 'those', 'of', 'the', 'terrible', 'storms', ',', 'in', 'the', 'various', 'countries', 'of', 'the', 'European', 'Union', '.']\n",
      "Some tokenized examples from the imported dataset:\n",
      "['Ripresa', 'della', 'sessione']\n",
      "['Dichiaro', 'ripresa', 'la', 'sessione', 'del', 'Parlamento', 'europeo', ',', 'interrotta', 'venerdì', '17', 'dicembre', 'e', 'rinnovo', 'a', 'tutti', 'i', 'miei', 'migliori', 'auguri', 'nella', 'speranza', 'che', 'abbiate', 'trascorso', 'delle', 'buone', 'vacanze', '.']\n",
      "['Come', 'avrete', 'avuto', 'modo', 'di', 'constatare', 'il', 'grande', '``', 'baco', 'del', 'millennio', \"''\", 'non', 'si', 'è', 'materializzato', '.', 'Invece', ',', 'i', 'cittadini', 'di', 'alcuni', 'nostri', 'paesi', 'sono', 'stati', 'colpiti', 'da', 'catastrofi', 'naturali', 'di', 'proporzioni', 'davvero', 'terribili', '.']\n",
      "['Avete', 'chiesto', 'che', 'si', 'tenesse', 'una', 'discussione', 'su', 'tale', 'tema', 'nei', 'prossimi', 'giorni', ',', 'nel', 'corso', 'della', 'presente', 'tornata', '.']\n",
      "['Nel', 'frattempo', 'è', 'mio', 'desiderio', ',', 'come', 'del', 'resto', 'mi', 'è', 'stato', 'chiesto', 'da', 'alcuni', 'colleghi', ',', 'osservare', 'un', 'minuto', 'di', 'silenzio', 'in', 'memoria', 'di', 'tutte', 'le', 'vittime', 'delle', 'tempeste', 'che', 'si', 'sono', 'abbattute', 'sui', 'diversi', 'paesi', 'dell', \"'\", 'Unione', 'europea', '.']\n",
      "{'encoder_input': tensor([   1,  117,  351, 3778,   25, 1055, 9053,  686,   32, 2042,   94,   65,\n",
      "          30, 7051,  301,   75,  263,   34,  130,  105, 8587, 2632,   25,    6,\n",
      "        7786,    5,  471, 3002,   19,   94,   65,   21, 2025,    6, 1674,   94,\n",
      "        1404,   20, 1907, 1842,  392,   39,    2,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3]), 'decoder_input': tensor([    1,  7818, 25576,   345,  7441,   149,   603, 13060,   482,   472,\n",
      "           48,  1721,   156,   740,  9719,    13,  1969,   865,    68,  9719,\n",
      "        37866,   274,   412,     5,  8814,    39,    53,  1465,    13,    41,\n",
      "           28,  2987,    39,  1944,  2022,    96,  2210,   112,    96,  1514,\n",
      "           18,  2651,  2399,    34,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3]), 'encoder_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.int32), 'decoder_mask': tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]]]), 'label': tensor([ 7818, 25576,   345,  7441,   149,   603, 13060,   482,   472,    48,\n",
      "         1721,   156,   740,  9719,    13,  1969,   865,    68,  9719, 37866,\n",
      "          274,   412,     5,  8814,    39,    53,  1465,    13,    41,    28,\n",
      "         2987,    39,  1944,  2022,    96,  2210,   112,    96,  1514,    18,\n",
      "         2651,  2399,    34,     2,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3]), 'src': 'One last plea to Commissioner Byrne: in calling for this new documentation we should request that it be differentiated according to the composition of certain products, for this would reduce the expenditure for small and medium-sized businesses considerably.', 'trg': \"Un'ultima preghiera vorrei rivolgere al Commissario Byrne: quando si richiede questa nuova documentazione, dovremmo chiedere una documentazione differenziale sulla base della composizione di alcuni prodotti, il che consentirebbe di ridurre notevolmente le spese per le piccole e medie imprese.\"}\n",
      "Train dataset size: 1527292\n",
      "Validation dataset size: 190912\n",
      "Test dataset size: 190912\n",
      "\n",
      "Preprocessing english to spanish dataset:\n",
      "Some tokenized examples from the imported dataset:\n",
      "['Resumption', 'of', 'the', 'session']\n",
      "['I', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'European', 'Parliament', 'adjourned', 'on', 'Friday', '17', 'December', '1999', ',', 'and', 'I', 'would', 'like', 'on', 'ce', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period', '.']\n",
      "['Although', ',', 'as', 'you', 'will', 'have', 'seen', ',', 'the', 'dreaded', \"'millennium\", 'bug', \"'\", 'failed', 'to', 'materialise', ',', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful', '.']\n",
      "['You', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days', ',', 'during', 'this', 'part-session', '.']\n",
      "['In', 'the', 'meantime', ',', 'I', 'should', 'like', 'to', 'observe', 'a', 'minute', \"'\", 's', 'silence', ',', 'as', 'a', 'number', 'of', 'Members', 'have', 'requested', ',', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned', ',', 'particularly', 'those', 'of', 'the', 'terrible', 'storms', ',', 'in', 'the', 'various', 'countries', 'of', 'the', 'European', 'Union', '.']\n",
      "Some tokenized examples from the imported dataset:\n",
      "['Reanudación', 'del', 'período', 'de', 'sesiones']\n",
      "['Declaro', 'reanudado', 'el', 'período', 'de', 'sesiones', 'del', 'Parlamento', 'Europeo', ',', 'interrumpido', 'el', 'viernes', '17', 'de', 'diciembre', 'pasado', ',', 'y', 'reitero', 'a', 'Sus', 'Señorías', 'mi', 'deseo', 'de', 'que', 'hayan', 'tenido', 'unas', 'buenas', 'vacaciones', '.']\n",
      "['Como', 'todos', 'han', 'podido', 'comprobar', ',', 'el', 'gran', '``', 'efecto', 'del', 'año', '2000', \"''\", 'no', 'se', 'ha', 'producido', '.', 'En', 'cambio', ',', 'los', 'ciudadanos', 'de', 'varios', 'de', 'nuestros', 'países', 'han', 'sido', 'víctimas', 'de', 'catástrofes', 'naturales', 'verdaderamente', 'terribles', '.']\n",
      "['Sus', 'Señorías', 'han', 'solicitado', 'un', 'debate', 'sobre', 'el', 'tema', 'para', 'los', 'próximos', 'días', ',', 'en', 'el', 'curso', 'de', 'este', 'período', 'de', 'sesiones', '.']\n",
      "['A', 'la', 'espera', 'de', 'que', 'se', 'produzca', ',', 'de', 'acuerdo', 'con', 'muchos', 'colegas', 'que', 'me', 'lo', 'han', 'pedido', ',', 'pido', 'que', 'hagamos', 'un', 'minuto', 'de', 'silencio', 'en', 'memoria', 'de', 'todas', 'las', 'víctimas', 'de', 'las', 'tormentas', ',', 'en', 'los', 'distintos', 'países', 'de', 'la', 'Unión', 'Europea', 'afectados', '.']\n",
      "{'encoder_input': tensor([    1,  2819,    20,   922,    19,   121,   101,    19,   100,   920,\n",
      "           19,    40, 11686,  4852,   300,    74,   230,  3557,  4824,     5,\n",
      "            6,   386,    33,   228,   300,   323,  1297,   244,   322,   213,\n",
      "           27,  6377,   645,  1597,    27,   245,    20,  1903,   808,  6377,\n",
      "          645,  6441,  1350,   181,   166,    27,  2133,  1311,   106,   552,\n",
      "           33,   300,    31, 11686,  1061,    42,  2276,  4100,    38,     2,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3]), 'decoder_input': tensor([    1, 18387,    20,  2943,    14,   159,   135,    14,  1073,  1072,\n",
      "           14,   163,   385, 17120,  1198,    27,   912,    70,   144,   245,\n",
      "            7,  2351,    27,   788,  1760,    22,    63,  1552,     7,  2237,\n",
      "         1591,  1060,    70,   144,   324,    74,  1137,   327,    27,    46,\n",
      "         1367,    14,    20,  2283,  1222,    74,  3080,  1060,    27,   297,\n",
      "          107,  2583,  5144,     7,  6258,    27,   307,  2803,  1222,   424,\n",
      "           70,  1218, 19085,    33,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3]), 'encoder_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.int32), 'decoder_mask': tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]]]), 'label': tensor([18387,    20,  2943,    14,   159,   135,    14,  1073,  1072,    14,\n",
      "          163,   385, 17120,  1198,    27,   912,    70,   144,   245,     7,\n",
      "         2351,    27,   788,  1760,    22,    63,  1552,     7,  2237,  1591,\n",
      "         1060,    70,   144,   324,    74,  1137,   327,    27,    46,  1367,\n",
      "           14,    20,  2283,  1222,    74,  3080,  1060,    27,   297,   107,\n",
      "         2583,  5144,     7,  6258,    27,   307,  2803,  1222,   424,    70,\n",
      "         1218, 19085,    33,     2,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3]), 'src': 'Ladies and gentlemen, Mr President, Madam Commissioner, as Western Europeans we should not lose sight of the fact that what we are dealing with here is a unique - certainly a first and perhaps even unique - transformation process which has a dimension different from any that we in Western Europe have hitherto experienced.', 'trg': 'Señoras y señores, señor Presidente, distinguida Comisaria, como europeos occidentales tenemos que tratar en todo momento de recordar que estamos frente a un proceso de transformación único - en todo caso la primera vez que se da, y quizás hasta la única - que tiene una dimensión distinta de aquella que hemos conocido hasta ahora en Europa Occidental.'}\n",
      "Train dataset size: 1572587\n",
      "Validation dataset size: 196574\n",
      "Test dataset size: 196574\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the datasets\n",
    "print(\"Preprocessing english to italian dataset:\")\n",
    "preprocess_datasets(config['en-it-dataset-english'] , config['en-it-dataset-italian'], config['en-it-save-path'])\n",
    "\n",
    "print()\n",
    "print(\"Preprocessing english to spanish dataset:\")\n",
    "preprocess_datasets(config['en-es-dataset-english'], config['en-es-dataset-spanish'], config['en-es-save-path'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
