{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the set of imports that we will use to train the model and import the dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json # for loading the json file\n",
    "\n",
    "# Loading dataset imports\n",
    "from torch.utils.data import DataLoader # for creating the dataloader\n",
    "\n",
    "# Training imports\n",
    "from Transformer_model import build_transformer # the model\n",
    "from torch.utils.tensorboard import SummaryWriter  # for logging during training\n",
    "from tqdm import tqdm # for the progress bar during training\n",
    "\n",
    "# For saving the model and checkpoints during training\n",
    "import os\n",
    "\n",
    "# For calculating the BLEU score \n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the hyperparameters that we will use to train the model. These are defined in the `hyperparameters.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON files\n",
    "def load_json(json_file):\n",
    "    with open(json_file) as json_data:\n",
    "        d = json.load(json_data)\n",
    "        return d\n",
    "    \n",
    "hyperparameters = load_json('hyperparameters.json')\n",
    "\n",
    "# Load the hyperparameters\n",
    "learning_rates = hyperparameters[\"learning_rates\"]\n",
    "epochs = hyperparameters[\"num_epochs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import English to Italian Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We first load the english to italian translation datasets we created by runnning the `Preprocessing.ipynb` file (training, validation and test datasets). We also import the vocabulary dictionaries for both the source and the target languages (also saved from running the `Preprocessing.ipynb` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_file = load_json('header.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the datasets using the paths defined in the header file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset:  15999\n",
      "Size of validation dataset:  2001\n",
      "Size of test dataset:  2000\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets path from the header file\n",
    "en_it_dataset_path = header_file['en-it-save-path']\n",
    "\n",
    "# Load the datasets\n",
    "en_it_train = torch.load(en_it_dataset_path + 'train_ds.pt')\n",
    "en_it_val = torch.load(en_it_dataset_path + 'val_ds.pt')\n",
    "en_it_test = torch.load(en_it_dataset_path + 'test_ds.pt')\n",
    "\n",
    "# Load the vocabularies from the header file\n",
    "en_it_source_vocab = torch.load(en_it_dataset_path + 'source_vocab.pt')\n",
    "en_it_target_vocab = torch.load(en_it_dataset_path + 'target_vocab.pt')\n",
    "\n",
    "# Print the size of the dataset as a sanity check\n",
    "print('Size of training dataset: ', len(en_it_train))\n",
    "print('Size of validation dataset: ', len(en_it_val))\n",
    "print('Size of test dataset: ', len(en_it_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the dataloaders for the training, validation and test datasets we just imported. We use the dataloaders to create batches of data that we will use to train the model. The batch size is defined in the `hyperparameters.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for the datasets. The batch size is specified in the hyperparameters file\n",
    "en_it_train_dl = DataLoader(en_it_train, batch_size=hyperparameters[\"batch_size\"], shuffle=True)\n",
    "en_it_val_dl = DataLoader(en_it_val, batch_size=1, shuffle=False) # batch size is 1 for validation\n",
    "en_it_test_dl = DataLoader(en_it_test, batch_size=1, shuffle=False) # batch size is 1 for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the English to Spanish Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We then load the english to spanish translation datasets that was created using the same method as the english to italian datasets. We also import the vocabulary dictionaries for both the source and the target languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset:  15999\n",
      "Size of validation dataset:  2001\n",
      "Size of test dataset:  2000\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets path from the header file\n",
    "en_es_dataset_path = header_file['en-es-save-path']\n",
    "\n",
    "# Load the datasets\n",
    "en_es_train = torch.load(en_es_dataset_path + 'train_ds.pt')\n",
    "en_es_val = torch.load(en_es_dataset_path + 'val_ds.pt')\n",
    "en_es_test = torch.load(en_es_dataset_path + 'test_ds.pt')\n",
    "\n",
    "# Load the vocabularies from the header file\n",
    "en_es_source_vocab = torch.load(en_es_dataset_path + 'source_vocab.pt')\n",
    "en_es_target_vocab = torch.load(en_es_dataset_path + 'target_vocab.pt')\n",
    "\n",
    "# Print the size of the dataset as a sanity check\n",
    "print('Size of training dataset: ', len(en_es_train))\n",
    "print('Size of validation dataset: ', len(en_es_val))\n",
    "print('Size of test dataset: ', len(en_es_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the dataloaders for the training, validation and test datasets we just imported. We use the dataloaders to create batches of data that we will use to train the model. The batch size is defined in the `hyperparameters.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for the datasets. The batch size is specified in the hyperparameters file\n",
    "en_es_train_dl = DataLoader(en_es_train, batch_size=hyperparameters[\"batch_size\"], shuffle=True)\n",
    "en_es_val_dl = DataLoader(en_es_val, batch_size=1, shuffle=False) # batch size is 1 for validation\n",
    "en_es_test_dl = DataLoader(en_es_test, batch_size=1, shuffle=False) # batch size is 1 for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the device to be used for training. We use the GPU if it is available, otherwise we use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Select device: cuda, mps or cpu\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Causal mask: each word in the decoder can only look at previous words\n",
    "    This is done to prevent the decoder from looking at future words.\n",
    "    \"\"\"\n",
    "    # Create a matrix of size seq_len x seq_len\n",
    "    # Fill the upper triangle with 0s and lower triangle with 1s\n",
    "    # This is done to prevent the decoder from looking at future words\n",
    "    return torch.tril(torch.ones((1, seq_len, seq_len), dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens(token_ids, vocab):\n",
    "    \"\"\"\n",
    "    Decode a list of token IDs back to a sentence using the vocabulary.\n",
    "    \"\"\"\n",
    "    # Create a reverse vocabulary\n",
    "    reverse_vocab = {id: word for word, id in vocab.items()}\n",
    "\n",
    "    # Decode the token IDs to words\n",
    "    words = [reverse_vocab.get(id, \"[UNK]\") for id in token_ids]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, encoder_mask, trg_vocab, sos_idx, eos_idx, max_len, device):    \n",
    "    # Precompute the encoder output and reuse it for every token we get from the decoder\n",
    "    encoder_output = model.encode(source, encoder_mask)\n",
    "\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
    "    \n",
    "    while True:\n",
    "        # Break if the decoder input size is equal to the max length (which is set in the header file)\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "        \n",
    "        # Create a mask to prevent the decoder from looking at future words\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n",
    "\n",
    "        # Get the decoder output\n",
    "        decoder_output = model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask)     \n",
    "        \n",
    "        # Get the last predicted token\n",
    "        output = model.output(decoder_output[:, -1])\n",
    "\n",
    "        # Get the token with the max probability (greedy search)\n",
    "        _, next_word = torch.max(output, dim=1)\n",
    "\n",
    "        # Concatenate the predicted token to the decoder input as the next input for the decoder\n",
    "        decoder_input = torch.cat([decoder_input, torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
    "        \n",
    "        # Break if the decoder predicted the end of sentence token\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    # Remove the batch dimension\n",
    "    decoder_input = decoder_input.squeeze(0)\n",
    "    \n",
    "    # Convert the decoded sentence to a list of token IDs \n",
    "    decoder_input = decoder_input.detach().cpu().numpy()\n",
    "\n",
    "    # Remove the sos token from the decoded sentence\n",
    "    decoder_input = decoder_input[1:]\n",
    "\n",
    "    return decode_tokens(decoder_input, trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, lr, epochs, train_dl, val_dl, target_vocab, dataset_name):\n",
    "    # Define the tensorboard writer\n",
    "    writer = SummaryWriter() \n",
    "\n",
    "    # Define the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=1e-8)\n",
    "\n",
    "    # Define the loss function\n",
    "    # Ignore the padding token, which has index 3 in the vocabulary (see function build_vocab in Preprocessing.ipynb file)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=3).to(device)\n",
    "\n",
    "    # Define the checkpoint directory\n",
    "    checkpoint = os.path.join(dataset_name+\"checkpoints\", f\"lr_{lr}\")\n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(checkpoint, exist_ok=True)\n",
    "\n",
    "    # Find the latest checkpoint\n",
    "    latest_epoch = -1\n",
    "    latest_checkpoint_path = None\n",
    "    for fname in os.listdir(checkpoint):\n",
    "        if fname.startswith('epoch_') and fname.endswith('.pth'):\n",
    "            epoch_num = int(fname.split('_')[1].split('.')[0])\n",
    "            if epoch_num > latest_epoch:\n",
    "                latest_epoch = epoch_num\n",
    "                latest_checkpoint_path = os.path.join(checkpoint, fname)\n",
    "\n",
    "    if latest_checkpoint_path:\n",
    "        ckpt = torch.load(latest_checkpoint_path)\n",
    "        model.load_state_dict(ckpt['model_state'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state'])\n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        if start_epoch < epochs:\n",
    "            print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        else:\n",
    "            print(f\"Training already finished for parameter learning rate={lr}\")\n",
    "            return model\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    step = 0 # for logging the loss\n",
    "\n",
    "    for epoch in range (start_epoch, epochs):\n",
    "        # Empty the cache to avoid memory overflow\n",
    "        torch.mps.empty_cache()        \n",
    "        # Set the model to train mode\n",
    "        model.train()\n",
    "\n",
    "        # Create the progress bar \n",
    "        iter = tqdm(train_dl, desc=f'Epoch {epoch}')\n",
    "\n",
    "        # Iterate over the batches\n",
    "        for batch in iter:\n",
    "            # Get the tensors from the batch\n",
    "            encoder_input = batch['encoder_input'].to(device)    # size (batch_size, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device)    # size (batch_size, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)      # size (batch_size, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device)      # size (batch_size, 1, seq_len, seq_len)\n",
    "            label = batch['label'].to(device)                    # size (batch_size, seq_len)\n",
    "\n",
    "            # Run the tensors through the model\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)                                   \n",
    "            decoder_output = model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask)     \n",
    "            output = model.output(decoder_output)                                                        # size (batch_size, seq_len, trg_vocab_size)\n",
    "\n",
    "            # Calculate the loss\n",
    "            # Flatten the output and label tensors to size (batch_size * seq_len, trg_vocab_size)\n",
    "            train_loss = loss_fn(output.view(-1, len(target_vocab)), label.view(-1))\n",
    "            iter.set_postfix(loss=train_loss.item()) # print the loss\n",
    "            writer.add_scalar('Training Loss/Step', train_loss.item(), step) # log the loss\n",
    "            writer.flush() \n",
    "\n",
    "            # Backpropagation\n",
    "            train_loss.backward()    \n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            step += 1\n",
    "        \n",
    "        print(\"Evaluating the model on the validation dataset\")\n",
    "        # Evaluate the model on the validation dataset\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        # Disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                # Get the tensors from the batch\n",
    "                encoder_input = batch['encoder_input'].to(device)    \n",
    "                decoder_input = batch['decoder_input'].to(device)    \n",
    "                encoder_mask = batch['encoder_mask'].to(device)      \n",
    "                decoder_mask = batch['decoder_mask'].to(device)      \n",
    "                label = batch['label'].to(device)                    \n",
    "\n",
    "                # Run the tensors through the model\n",
    "                encoder_output = model.encode(encoder_input, encoder_mask)                                   \n",
    "                decoder_output = model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask)     \n",
    "                output = model.output(decoder_output)                                                        # size (batch_size, seq_len, trg_vocab_size)\n",
    "                \n",
    "                # Calculate the loss\n",
    "                # Flatten the output and label tensors to size (batch_size * seq_len, trg_vocab_size)\n",
    "                val_loss += loss_fn(output.view(-1, len(target_vocab)), label.view(-1)).item()\n",
    "\n",
    "        # Log the validation loss\n",
    "        val_loss /= len(val_dl)\n",
    "        writer.add_scalar('Validation Loss/Epoch', val_loss, epoch)\n",
    "        writer.flush()\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        epoch_checkpoint_path = os.path.join(str(checkpoint), f\"epoch_{epoch}_train_loss_{round(train_loss.item(), 2)}_val_loss_{round(val_loss, 2)}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state': model.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss\n",
    "        }, epoch_checkpoint_path)\n",
    "\n",
    "        print(f\"Checkpoint for epoch {epoch} saved\")\n",
    "        \n",
    "    writer.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_BLEU_score(model, val_dl, target_vocab, subset_size=100):\n",
    "    \"\"\"\n",
    "    Function to compute the BLEU score on the validation dataset. \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Initialize the lists that will contain the references and the outputs for each sentence when computing the BLEU score\n",
    "    references = []\n",
    "    outputs = []\n",
    "    count = 0\n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dl:\n",
    "            if count >= subset_size:\n",
    "                break\n",
    "            # Get the tensors from the batch\n",
    "            encoder_input = batch['encoder_input'].to(device)    \n",
    "            encoder_mask = batch['encoder_mask'].to(device)      \n",
    "            \n",
    "            # Generate the translation\n",
    "            translation = greedy_decode(\n",
    "                model=model,\n",
    "                source=encoder_input,\n",
    "                encoder_mask=encoder_mask,\n",
    "                trg_vocab = target_vocab,\n",
    "                sos_idx=1,\n",
    "                eos_idx=2,\n",
    "                max_len=header_file['max_seq_len'],\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            count += len(batch['encoder_input'])\n",
    "            \n",
    "        # Add the generated translation and the reference translation to the lists\n",
    "        outputs.append(translation)\n",
    "        references.append(batch['trg'])  # Assuming this is available in your validation DataLoader\n",
    "\n",
    "    # Compute the BLEU score and return it\n",
    "    return corpus_bleu(references, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the English to Italian Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1000/1000 [09:25<00:00,  1.77it/s, loss=5.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 0 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1000/1000 [08:39<00:00,  1.93it/s, loss=4.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 1 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1000/1000 [08:34<00:00,  1.94it/s, loss=4.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 2 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1000/1000 [08:32<00:00,  1.95it/s, loss=3.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 3 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1000/1000 [08:33<00:00,  1.95it/s, loss=3.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 4 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1000/1000 [08:28<00:00,  1.97it/s, loss=2.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 5 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1000/1000 [08:30<00:00,  1.96it/s, loss=2.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 6 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1000/1000 [08:31<00:00,  1.95it/s, loss=2.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 7 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1000/1000 [08:28<00:00,  1.97it/s, loss=2.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 8 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1000/1000 [08:25<00:00,  1.98it/s, loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 9 saved\n",
      "Learning rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1000/1000 [08:27<00:00,  1.97it/s, loss=5.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 0 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1000/1000 [08:21<00:00,  2.00it/s, loss=5.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 1 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1000/1000 [08:20<00:00,  2.00it/s, loss=4.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 2 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1000/1000 [08:20<00:00,  2.00it/s, loss=4.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 3 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1000/1000 [08:26<00:00,  1.97it/s, loss=3.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 4 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1000/1000 [08:20<00:00,  2.00it/s, loss=4.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 5 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1000/1000 [08:20<00:00,  2.00it/s, loss=3.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 6 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1000/1000 [08:22<00:00,  1.99it/s, loss=3.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 7 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1000/1000 [08:19<00:00,  2.00it/s, loss=2.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 8 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1000/1000 [08:20<00:00,  2.00it/s, loss=2.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 9 saved\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "    # Define the model\n",
    "    model = build_transformer(\n",
    "                        len(en_it_source_vocab),                    # size of the source vocabulary    \n",
    "                        len(en_it_target_vocab),                    # size of the target vocabulary\n",
    "                        src_seq_len= header_file[\"max_seq_len\"],    # defined in the header file \n",
    "                        trg_seq_len= header_file[\"max_seq_len\"],    # defined in the header file\n",
    "                        d_model = 512,                              # based on the paper\n",
    "                        N = 3,                                      # number of encoder and decoder layers (we use the same number of layers for both encoder and decoder)\n",
    "                        h = 8,                                      # number of heads (we use the same number of heads for both encoder and decoder)                                          \n",
    "                        dropout = 0.1,                              # based on the paper\n",
    "                        d_ff = 2048                                 # based on the paper\n",
    "                        ).to(device)    \n",
    "    print('Learning rate:', lr)\n",
    "    model = train(model, lr, epochs, en_it_train_dl, en_it_val_dl, en_it_target_vocab, \"en_it_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the English to Spanish Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1000/1000 [08:22<00:00,  1.99it/s, loss=5.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 0 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1000/1000 [08:17<00:00,  2.01it/s, loss=3.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 1 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1000/1000 [08:15<00:00,  2.02it/s, loss=3.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 2 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1000/1000 [08:12<00:00,  2.03it/s, loss=3.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 3 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1000/1000 [08:20<00:00,  2.00it/s, loss=2.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 4 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1000/1000 [08:14<00:00,  2.02it/s, loss=2.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 5 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1000/1000 [08:15<00:00,  2.02it/s, loss=2.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 6 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1000/1000 [08:18<00:00,  2.01it/s, loss=2.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 7 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1000/1000 [08:14<00:00,  2.02it/s, loss=2.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 8 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1000/1000 [08:13<00:00,  2.03it/s, loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 9 saved\n",
      "Learning rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1000/1000 [08:12<00:00,  2.03it/s, loss=4.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 0 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1000/1000 [08:14<00:00,  2.02it/s, loss=4.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 1 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1000/1000 [08:10<00:00,  2.04it/s, loss=4.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 2 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1000/1000 [08:09<00:00,  2.04it/s, loss=4.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 3 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1000/1000 [08:12<00:00,  2.03it/s, loss=3.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 4 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1000/1000 [08:10<00:00,  2.04it/s, loss=3.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 5 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1000/1000 [08:10<00:00,  2.04it/s, loss=2.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 6 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1000/1000 [08:11<00:00,  2.03it/s, loss=2.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 7 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1000/1000 [08:11<00:00,  2.03it/s, loss=2.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 8 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1000/1000 [08:10<00:00,  2.04it/s, loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 9 saved\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "    # Define the model\n",
    "    model = build_transformer(\n",
    "                        len(en_es_source_vocab),                    # size of the source vocabulary    \n",
    "                        len(en_es_target_vocab),                    # size of the target vocabulary\n",
    "                        src_seq_len= header_file[\"max_seq_len\"],    # defined in the header file \n",
    "                        trg_seq_len= header_file[\"max_seq_len\"],    # defined in the header file\n",
    "                        d_model = 512,                              # based on the paper\n",
    "                        N = 3,                                      # number of encoder and decoder layers (we use the same number of layers for both encoder and decoder)\n",
    "                        h = 8,                                      # number of heads (we use the same number of heads for both encoder and decoder)                                          \n",
    "                        dropout = 0.1,                              # based on the paper\n",
    "                        d_ff = 2048                                 # based on the paper\n",
    "                        ).to(device)    \n",
    "    print('Learning rate:', lr)\n",
    "    model = train(model, lr, epochs, en_es_train_dl, en_es_val_dl, en_es_target_vocab, \"en_es_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the models on the validation sets to pick the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Italian Model for learning rate 0.0001 and epoch 9 loaded\n",
      "English Italian Model for learning rate 0.001 and epoch 9 loaded\n",
      "English Spanish Model for learning rate 0.0001 and epoch 9 loaded\n",
      "English Spanish Model for learning rate 0.001 and epoch 9 loaded\n"
     ]
    }
   ],
   "source": [
    "# Create dictionaries to store the models for each learning rate\n",
    "en_it_models = {}\n",
    "en_es_models = {}\n",
    "\n",
    "# Load the last epochs of the models for each learning rate in their corresponding dictionaries\n",
    "for lr in learning_rates:\n",
    "    model = build_transformer(\n",
    "                    len(en_it_source_vocab),                    # size of the source vocabulary    \n",
    "                    len(en_it_target_vocab),                    # size of the target vocabulary\n",
    "                    src_seq_len= header_file[\"max_seq_len\"],    # defined in the header file \n",
    "                    trg_seq_len= header_file[\"max_seq_len\"],    # defined in the header file\n",
    "                    d_model = 512,                              # based on the paper\n",
    "                    N = 3,                                      # number of encoder and decoder layers (we use the same number of layers for both encoder and decoder)\n",
    "                    h = 8,                                      # number of heads (we use the same number of heads for both encoder and decoder)                                          \n",
    "                    dropout = 0.1,                              # based on the paper\n",
    "                    d_ff = 2048                                 # based on the paper\n",
    "                    ).to(device) \n",
    "        \n",
    "    en_it_checkpoint = os.path.join(\"en_it_checkpoints\", f\"lr_{lr}\")    \n",
    "    latest_epoch = -1\n",
    "    for fname in os.listdir(en_it_checkpoint):\n",
    "        if fname.startswith('epoch_') and fname.endswith('.pth'):\n",
    "            epoch_num = int(fname.split('_')[1].split('.')[0])\n",
    "            if epoch_num > latest_epoch:\n",
    "                latest_epoch = epoch_num\n",
    "                latest_checkpoint_path = os.path.join(en_it_checkpoint, fname)\n",
    "\n",
    "    if latest_checkpoint_path:\n",
    "        ckpt = torch.load(latest_checkpoint_path)\n",
    "        model.load_state_dict(ckpt['model_state'])\n",
    "        en_it_models[lr] = model\n",
    "        print(f\"English Italian Model for learning rate {lr} and epoch {latest_epoch} loaded\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = build_transformer(\n",
    "                len(en_es_source_vocab),                    # size of the source vocabulary    \n",
    "                len(en_es_target_vocab),                    # size of the target vocabulary\n",
    "                src_seq_len= header_file[\"max_seq_len\"],    # defined in the header file \n",
    "                trg_seq_len= header_file[\"max_seq_len\"],    # defined in the header file\n",
    "                d_model = 512,                              # based on the paper\n",
    "                N = 3,                                      # number of encoder and decoder layers (we use the same number of layers for both encoder and decoder)\n",
    "                h = 8,                                      # number of heads (we use the same number of heads for both encoder and decoder)                                          \n",
    "                dropout = 0.1,                              # based on the paper\n",
    "                d_ff = 2048                                 # based on the paper\n",
    "                ).to(device) \n",
    "    \n",
    "    en_es_checkpoint = os.path.join(\"en_es_checkpoints\", f\"lr_{lr}\")    \n",
    "    latest_epoch = -1\n",
    "    for fname in os.listdir(en_es_checkpoint):\n",
    "        if fname.startswith('epoch_') and fname.endswith('.pth'):\n",
    "            epoch_num = int(fname.split('_')[1].split('.')[0])\n",
    "            if epoch_num > latest_epoch:\n",
    "                latest_epoch = epoch_num\n",
    "                latest_checkpoint_path = os.path.join(en_es_checkpoint, fname)\n",
    "\n",
    "    if latest_checkpoint_path:\n",
    "        ckpt = torch.load(latest_checkpoint_path)\n",
    "        model.load_state_dict(ckpt['model_state'])\n",
    "        en_es_models[lr] = model\n",
    "        print(f\"English Spanish Model for learning rate {lr} and epoch {latest_epoch} loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU scores on the english-italian models trained with different learning rates:\n",
      "Learning rate: 0.0001\n",
      "BLEU score: 0.03795893975135081\n",
      "Learning rate: 0.001\n",
      "BLEU score: 0.036117260153593664\n"
     ]
    }
   ],
   "source": [
    "best_en_it_model = None\n",
    "best_BLEU = -1\n",
    "best_en_it_lr = None\n",
    "# Compare the BLEU scores on the models trained with different learning rates\n",
    "print('BLEU scores on the english-italian models trained with different learning rates:')\n",
    "for lr in learning_rates:\n",
    "    print('Learning rate:', lr)\n",
    "\n",
    "    # Compute the BLEU score on the validation dataset\n",
    "    BLEU = compute_BLEU_score(en_it_models[lr], en_it_val_dl, en_it_target_vocab)\n",
    "    print('BLEU score:', BLEU)\n",
    "    if BLEU > best_BLEU:\n",
    "        best_BLEU = BLEU\n",
    "        best_en_it_model = en_it_models[lr]\n",
    "        best_en_it_lr = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU scores on the english-spanish models trained with different learning rates:\n",
      "Learning rate: 0.0001\n",
      "BLEU score: 0.08944820372220456\n",
      "Learning rate: 0.001\n",
      "BLEU score: 0.008682228073600117\n"
     ]
    }
   ],
   "source": [
    "best_en_es_model = None\n",
    "best_BLEU = -1\n",
    "best_en_es_lr = None\n",
    "# Compare the BLEU scores on the models trained with different learning rates\n",
    "print('BLEU scores on the english-spanish models trained with different learning rates:')\n",
    "for lr in learning_rates:\n",
    "    print('Learning rate:', lr)\n",
    "    # Compute the BLEU score on the validation dataset\n",
    "    BLEU = compute_BLEU_score(en_es_models[lr], en_es_val_dl, en_es_target_vocab)\n",
    "    print('BLEU score:', BLEU)\n",
    "    if BLEU > best_BLEU:\n",
    "        best_BLEU = BLEU\n",
    "        best_en_es_model = en_es_models[lr]\n",
    "        best_en_es_lr = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune the English to Spanish Model on the English to Italian Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights (Model Before Transfer):\n",
      "Encoder Layer 0, First Parameter Mean and Std: -0.00012798531679436564 0.044176071882247925\n",
      "Decoder Layer 0, First Parameter Mean and Std: -0.00019926796085201204 0.04417480155825615\n",
      "Transferred Weights (Model After Transfer):\n",
      "Encoder Layer 0, First Parameter Mean and Std: -3.552586349542253e-05 0.04578001797199249\n",
      "Decoder Layer 0, First Parameter Mean and Std: -5.2119379688519984e-05 0.04419827088713646\n"
     ]
    }
   ],
   "source": [
    "# Assuming `english_spanish_model` is your pre-trained English-Spanish model.\n",
    "# Initialize a new model for English-Italian with the new target vocabulary size.\n",
    "model = build_transformer(\n",
    "                    len(en_it_source_vocab),                    # size of the source vocabulary    \n",
    "                    len(en_it_target_vocab),                    # size of the target vocabulary\n",
    "                    src_seq_len= header_file[\"max_seq_len\"],    # defined in the header file \n",
    "                    trg_seq_len= header_file[\"max_seq_len\"],    # defined in the header file\n",
    "                    d_model = 512,                              # based on the paper\n",
    "                    N = 3,                                      # number of encoder and decoder layers (we use the same number of layers for both encoder and decoder)\n",
    "                    h = 8,                                      # number of heads (we use the same number of heads for both encoder and decoder)                                          \n",
    "                    dropout = 0.1,                              # based on the paper\n",
    "                    d_ff = 2048                                 # based on the paper\n",
    "                    ).to(device)  \n",
    "\n",
    "# Print the initial weights of some layers as a sanity check\n",
    "print(\"Initial Weights (Model Before Transfer):\")\n",
    "print(\"Encoder Layer 0, First Parameter Mean and Std:\", torch.mean(model.encoder.n_layers[0].self_attention_block.W_Q.weight).item(), torch.std(model.encoder.n_layers[0].self_attention_block.W_Q.weight).item())\n",
    "print(\"Decoder Layer 0, First Parameter Mean and Std:\", torch.mean(model.decoder.n_layers[0].self_attention_block.W_Q.weight).item(), torch.std(model.decoder.n_layers[0].self_attention_block.W_Q.weight).item())\n",
    "\n",
    "# Transfer weights from the English-Spanish model to the new English-Italian model (only the encoder and decoder layers).\n",
    "# We will exclude the final linear layer's weights since the vocabulary size is different.\n",
    "# Transfer encoder weights.\n",
    "for it_src, it_tgt in zip(best_en_es_model.encoder.n_layers.parameters(), model.encoder.n_layers.parameters()):\n",
    "    it_tgt.data.copy_(it_src.data)\n",
    "\n",
    "# Transfer decoder weights, except for the last linear layer.\n",
    "for it_src, it_tgt in zip(best_en_es_model.decoder.n_layers.parameters(), model.decoder.n_layers.parameters()):\n",
    "    if it_tgt.size() == it_src.size():\n",
    "        it_tgt.data.copy_(it_src.data)\n",
    "\n",
    "# Reinitialize the final linear layer's weights\n",
    "model.linear_layer.proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "model.linear_layer.proj.bias.data.zero_()\n",
    "\n",
    "# Print the initial weights of some layers as a sanity check\n",
    "print(\"Transferred Weights (Model After Transfer):\")\n",
    "print(\"Encoder Layer 0, First Parameter Mean and Std:\", torch.mean(model.encoder.n_layers[0].self_attention_block.W_Q.weight).item(), torch.std(model.encoder.n_layers[0].self_attention_block.W_Q.weight).item())\n",
    "print(\"Decoder Layer 0, First Parameter Mean and Std:\", torch.mean(model.decoder.n_layers[0].self_attention_block.W_Q.weight).item(), torch.std(model.decoder.n_layers[0].self_attention_block.W_Q.weight).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1000/1000 [08:32<00:00,  1.95it/s, loss=4.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 0 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1000/1000 [08:17<00:00,  2.01it/s, loss=3.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 1 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1000/1000 [08:17<00:00,  2.01it/s, loss=3.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 2 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1000/1000 [08:20<00:00,  2.00it/s, loss=2.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 3 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1000/1000 [08:17<00:00,  2.01it/s, loss=2.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 4 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1000/1000 [08:17<00:00,  2.01it/s, loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 5 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1000/1000 [08:19<00:00,  2.00it/s, loss=1.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 6 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1000/1000 [08:18<00:00,  2.01it/s, loss=1.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 7 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1000/1000 [08:17<00:00,  2.01it/s, loss=1.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 8 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1000/1000 [08:17<00:00,  2.01it/s, loss=1.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 9 saved\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_en_es_model = train(model, best_en_es_lr, epochs, en_it_train_dl, en_it_val_dl, en_it_target_vocab, \"fine_tuned_en_es_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the BLEU scores on the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionnary to save the BLEU score results of the fine-tuned model and the original english to italian model\n",
    "BLEU_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score on the test dataset of the English-Italian model is: 0.02300914814791133\n"
     ]
    }
   ],
   "source": [
    "# Compute the BLEU score of the best english to italian model on the test dataset\n",
    "BLEU = compute_BLEU_score(best_en_it_model, en_it_test_dl, en_it_target_vocab, len(en_it_test))\n",
    "print('BLEU score on the test dataset of the English-Italian model is:', BLEU)\n",
    "BLEU_scores['en_it'] = BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score on the test dataset of the fine tuned English-Spanish model on the English-Italian task is: 0.042610063363542866\n"
     ]
    }
   ],
   "source": [
    "# Compute the BLEU score of the fine tuned English-Spanish model on the English-Italian task\n",
    "BLEU = compute_BLEU_score(fine_tuned_en_es_model, en_it_test_dl, en_it_target_vocab, len(en_it_test))\n",
    "print('BLEU score on the test dataset of the fine tuned English-Spanish model on the English-Italian task is:', BLEU)\n",
    "BLEU_scores['fine_tuned_en_es_'] = BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dicitonnary containing the BLEU scores in a json file\n",
    "with open('./results/BLEU_scores.json', 'w') as fp:\n",
    "    json.dump(BLEU_scores, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine tuned model and the original english to italian model in the results folder\n",
    "torch.save(best_en_it_model, './results/best_en_it_model.pt')\n",
    "torch.save(fine_tuned_en_es_model, './results/fine_tuned_en_es_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
