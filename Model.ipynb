{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/enzobenoit-\n",
      "[nltk_data]     jeannin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/Users/enzobenoit-jeannin/Documents/GitHub/COMP550/myenv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Loading dataset imports\n",
    "from torch.utils.data import Dataset, DataLoader # for creating the dataloader\n",
    "import json # for loading the json file\n",
    "from TranslationDataset import TranslationDataset # the custom dataset class\n",
    "\n",
    "\n",
    "# Training imports\n",
    "from Transformer_model import Transformer, build_transformer # the model\n",
    "from torch.utils.tensorboard import SummaryWriter  # for logging during training\n",
    "from tqdm import tqdm # for the progress bar during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import English to Italian Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We first load the english to italian translation datasets we created by runnning the Preprocessing.ipynb file (training, validation, test). We also import the vocabulary dictionaries for both languagesÂ (also saved from running the Preprocessing.ipynb file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON header file\n",
    "def load_json_header(json_file):\n",
    "    with open(json_file) as json_data:\n",
    "        d = json.load(json_data)\n",
    "        return d\n",
    "\n",
    "config = load_json_header('config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset:  1527292\n",
      "Size of validation dataset:  190912\n",
      "Size of test dataset:  190912\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "# Get the dataset path from the config file\n",
    "en_it_dataset_path = config['en-it-save-path']\n",
    "\n",
    "# Load the dataset\n",
    "en_it_train = torch.load(en_it_dataset_path + 'train_ds.pt')\n",
    "en_it_val = torch.load(en_it_dataset_path + 'val_ds.pt')\n",
    "en_it_test = torch.load(en_it_dataset_path + 'test_ds.pt')\n",
    "\n",
    "# Load the vocabularies from the config file\n",
    "source_vocab = torch.load(en_it_dataset_path + 'source_vocab.pt')\n",
    "target_vocab = torch.load(en_it_dataset_path + 'target_vocab.pt')\n",
    "\n",
    "# Print the size of the dataset\n",
    "print('Size of training dataset: ', len(en_it_train))\n",
    "print('Size of validation dataset: ', len(en_it_val))\n",
    "print('Size of test dataset: ', len(en_it_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_dl = DataLoader(en_it_train, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_dl = DataLoader(en_it_val, batch_size=1, shuffle=False)\n",
    "test_dl = DataLoader(en_it_test, batch_size=config[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Select device: cuda, mps or cpu\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device('mps')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hyperparams):\n",
    "    # Define the model\n",
    "    model = build_transformer(\n",
    "                        len(source_vocab),\n",
    "                        len(target_vocab),\n",
    "                        src_seq_len= config[\"max_seq_len\"],\n",
    "                        trg_seq_len= config[\"max_seq_len\"],\n",
    "                        d_model = 512,\n",
    "                        N = 1,\n",
    "                        h = 4,\n",
    "                        dropout = 0.1,\n",
    "                        d_ff = 2048).to(device)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Define the hyperparameters from the given dictionary\n",
    "    lr = hyperparams['lr']\n",
    "    epochs = hyperparams['epochs']\n",
    "\n",
    "    # Define the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['lr'], eps=1e-9)\n",
    "\n",
    "    # Define the loss function\n",
    "    # Ignore the padding token, which has index 3 in the vocabulary (see function build_vocab in Preprocessing.ipynb file)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=3, label_smoothing=0.1).to(device)\n",
    "\n",
    "    step = 0 # for logging the loss\n",
    "\n",
    "    for epoch in range (epochs):\n",
    "        torch.mps.empty_cache() # empty the cache\n",
    "        model.train()\n",
    "        iter = tqdm(train_dl, desc=f'Epoch {epoch}')\n",
    "        for batch in iter:\n",
    "            encoder_input = batch['encoder_input'].to(device) # size (batch_size, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # size (batch_size, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # size (batch_size, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # size (batch_size, 1, seq_len, seq_len)\n",
    "            label = batch['label'].to(device) # size (batch_size, seq_len)\n",
    "\n",
    "            # Run the tensors through the model\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)  # size (batch_size, seq_len, d_model)\n",
    "            decoder_output = model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask) # size (batch_size, seq_len, d_model)\n",
    "            output = model.output(decoder_output) # size (batch_size, seq_len, trg_vocab_size)\n",
    "\n",
    "            # Calculate the loss\n",
    "            # Flatten the output and label tensors to size (batch_size * seq_len, trg_vocab_size)\n",
    "            loss = loss_fn(output.view(-1, len(target_vocab)), label.view(-1))\n",
    "            iter.set_postfix(loss=loss.item()) # print the loss\n",
    "            writer.add_scalar('Loss/Step', loss.item(), step) # log the loss\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()    \n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'lr': 0.0001,\n",
    "    'epochs': 1\n",
    "}\n",
    "\n",
    "train(hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
