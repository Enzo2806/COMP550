{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Loading dataset imports\n",
    "from torch.utils.data import Dataset, DataLoader # for creating the dataloader\n",
    "import json # for loading the json file\n",
    "from TranslationDataset import TranslationDataset # the custom dataset class\n",
    "\n",
    "\n",
    "# Training imports\n",
    "from Transformer_model import Transformer, build_transformer # the model\n",
    "from torch.utils.tensorboard import SummaryWriter  # for logging during training\n",
    "from tqdm import tqdm # for the progress bar during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import English to Italian Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We first load the english to italian translation datasets we created by runnning the Preprocessing.ipynb file (training, validation, test). We also import the vocabulary dictionaries for both languagesÂ (also saved from running the Preprocessing.ipynb file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON header file\n",
    "def load_json_header(json_file):\n",
    "    with open(json_file) as json_data:\n",
    "        d = json.load(json_data)\n",
    "        return d\n",
    "\n",
    "config = load_json_header('config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset:  1527292\n",
      "Size of validation dataset:  190912\n",
      "Size of test dataset:  190912\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "# Get the dataset path from the config file\n",
    "en_it_dataset_path = config['en-it-save-path']\n",
    "\n",
    "# Load the dataset\n",
    "en_it_train = torch.load(en_it_dataset_path + 'train_ds.pt')\n",
    "en_it_val = torch.load(en_it_dataset_path + 'val_ds.pt')\n",
    "en_it_test = torch.load(en_it_dataset_path + 'test_ds.pt')\n",
    "\n",
    "# Load the vocabularies from the config file\n",
    "source_vocab = torch.load(en_it_dataset_path + 'source_vocab.pt')\n",
    "target_vocab = torch.load(en_it_dataset_path + 'target_vocab.pt')\n",
    "\n",
    "# Print the size of the dataset\n",
    "print('Size of training dataset: ', len(en_it_train))\n",
    "print('Size of validation dataset: ', len(en_it_val))\n",
    "print('Size of test dataset: ', len(en_it_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_dl = DataLoader(en_it_train, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_dl = DataLoader(en_it_val, batch_size=1, shuffle=False)\n",
    "test_dl = DataLoader(en_it_test, batch_size=config[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Select device: cuda, mps or cpu\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hyperparams):\n",
    "    # Define the model\n",
    "    model = build_transformer(\n",
    "                        len(source_vocab),\n",
    "                        len(target_vocab),\n",
    "                        src_seq_len= config[\"max_seq_len\"],\n",
    "                        trg_seq_len= config[\"max_seq_len\"],\n",
    "                        d_model = 512,\n",
    "                        N = 1,\n",
    "                        h = 4,\n",
    "                        dropout = 0.1,\n",
    "                        d_ff = 2048).to(device)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Define the hyperparameters from the given dictionary\n",
    "    lr = hyperparams['lr']\n",
    "    epochs = hyperparams['epochs']\n",
    "\n",
    "    # Define the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['lr'], eps=1e-9)\n",
    "\n",
    "    # Define the loss function\n",
    "    # Ignore the padding token, which has index 3 in the vocabulary (see function build_vocab in Preprocessing.ipynb file)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=3, label_smoothing=0.1).to(device)\n",
    "\n",
    "    step = 0 # for logging the loss\n",
    "\n",
    "    for epoch in range (epochs):\n",
    "        torch.mps.empty_cache() # empty the cache\n",
    "        model.train()\n",
    "        iter = tqdm(train_dl, desc=f'Epoch {epoch}')\n",
    "        for batch in iter:\n",
    "            encoder_input = batch['encoder_input'].to(device) # size (batch_size, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # size (batch_size, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # size (batch_size, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # size (batch_size, 1, seq_len, seq_len)\n",
    "            label = batch['label'].to(device) # size (batch_size, seq_len)\n",
    "\n",
    "            # Run the tensors through the model\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)  # size (batch_size, seq_len, d_model)\n",
    "            decoder_output = model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask) # size (batch_size, seq_len, d_model)\n",
    "            output = model.output(decoder_output) # size (batch_size, seq_len, trg_vocab_size)\n",
    "\n",
    "            # Calculate the loss\n",
    "            # Flatten the output and label tensors to size (batch_size * seq_len, trg_vocab_size)\n",
    "            loss = loss_fn(output.view(-1, len(target_vocab)), label.view(-1))\n",
    "            iter.set_postfix(loss=loss.item()) # print the loss\n",
    "            writer.add_scalar('Loss/Step', loss.item(), step) # log the loss\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()    \n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enzobenoit-jeannin/Documents/GitHub/COMP550/myenv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Epoch 0:   0%|          | 0/23864 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Transformer' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0001\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      4\u001b[0m }\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(hyperparams)\u001b[0m\n\u001b[1;32m     44\u001b[0m label \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# size (batch_size, seq_len)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Run the tensors through the model\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(encoder_input, encoder_mask)  \u001b[38;5;66;03m# size (batch_size, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode(decoder_input, encoder_output, encoder_mask, decoder_mask) \u001b[38;5;66;03m# size (batch_size, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39moutput(decoder_output) \u001b[38;5;66;03m# size (batch_size, seq_len, trg_vocab_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/COMP550/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Transformer' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    'lr': 0.0001,\n",
    "    'epochs': 1\n",
    "}\n",
    "\n",
    "train(hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
