{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the set of imports that we will use to train the model and import the dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enzobenoit-jeannin/Documents/GitHub/COMP550/myenv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json # for loading the json file\n",
    "\n",
    "# Loading dataset imports\n",
    "from torch.utils.data import DataLoader # for creating the dataloader\n",
    "\n",
    "# Training imports\n",
    "from Transformer_model import build_transformer # the model\n",
    "from torch.utils.tensorboard import SummaryWriter  # for logging during training\n",
    "from tqdm import tqdm # for the progress bar during training\n",
    "\n",
    "# For saving the model and checkpoints during training\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# For calculating the BLEU score \n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the hyperparameters that we will use to train the model. These are defined in the `hyperparameters.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON files\n",
    "def load_json(json_file):\n",
    "    with open(json_file) as json_data:\n",
    "        d = json.load(json_data)\n",
    "        return d\n",
    "    \n",
    "hyperparameters = load_json('hyperparameters.json')\n",
    "\n",
    "# Load the hyperparameters\n",
    "learning_rates = hyperparameters[\"learning_rates\"]\n",
    "epochs = hyperparameters[\"num_epochs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import English to Italian Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We first load the english to italian translation datasets we created by runnning the `Preprocessing.ipynb` file (training, validation and test datasets). We also import the vocabulary dictionaries for both the source and the target languages (also saved from running the `Preprocessing.ipynb` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_file = load_json('header.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the datasets using the paths defined in the header file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset:  15999\n",
      "Size of validation dataset:  2001\n",
      "Size of test dataset:  2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/enzobenoit-\n",
      "[nltk_data]     jeannin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets path from the header file\n",
    "en_it_dataset_path = header_file['en-it-save-path']\n",
    "\n",
    "# Load the datasets\n",
    "en_it_train = torch.load(en_it_dataset_path + 'train_ds.pt')\n",
    "en_it_val = torch.load(en_it_dataset_path + 'val_ds.pt')\n",
    "en_it_test = torch.load(en_it_dataset_path + 'test_ds.pt')\n",
    "\n",
    "# Load the vocabularies from the header file\n",
    "source_vocab = torch.load(en_it_dataset_path + 'source_vocab.pt')\n",
    "target_vocab = torch.load(en_it_dataset_path + 'target_vocab.pt')\n",
    "\n",
    "# Print the size of the dataset as a sanity check\n",
    "print('Size of training dataset: ', len(en_it_train))\n",
    "print('Size of validation dataset: ', len(en_it_val))\n",
    "print('Size of test dataset: ', len(en_it_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the dataloaders for the training, validation and test datasets we just imported. We use the dataloaders to create batches of data that we will use to train the model. The batch size is defined in the `hyperparameters.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for the datasets. The batch size is specified in the hyperparameters file\n",
    "train_dl = DataLoader(en_it_train, batch_size=hyperparameters[\"batch_size\"], shuffle=True)\n",
    "val_dl = DataLoader(en_it_val, batch_size=1, shuffle=False)         # batch size is 1 for validation and test\n",
    "test_dl = DataLoader(en_it_test, batch_size=1, shuffle=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the device to be used for training. We use the GPU if it is available, otherwise we use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Select device: cuda, mps or cpu\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Causal mask: each word in the decoder can only look at previous words\n",
    "    This is done to prevent the decoder from looking at future words.\n",
    "    \"\"\"\n",
    "    # Create a matrix of size seq_len x seq_len\n",
    "    # Fill the upper triangle with 0s and lower triangle with 1s\n",
    "    # This is done to prevent the decoder from looking at future words\n",
    "    return torch.tril(torch.ones((1, seq_len, seq_len), dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens(token_ids, vocab):\n",
    "    \"\"\"\n",
    "    Decode a list of token IDs back to a sentence using the vocabulary.\n",
    "    \"\"\"\n",
    "    # Create a reverse vocabulary\n",
    "    reverse_vocab = {id: word for word, id in vocab.items()}\n",
    "\n",
    "    # Decode the token IDs to words\n",
    "    words = [reverse_vocab.get(id, \"[UNK]\") for id in token_ids]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, encoder_mask, trg_vocab, sos_idx, eos_idx, max_len, device):    \n",
    "    # Precompute the encoder output and reuse it for every token we get from the decoder\n",
    "    encoder_output = model.encode(source, encoder_mask)\n",
    "\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
    "    \n",
    "    while True:\n",
    "        # Break if the decoder input size is equal to the max length (which is set in the header file)\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "        \n",
    "        # Create a mask to prevent the decoder from looking at future words\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n",
    "\n",
    "        # Get the decoder output\n",
    "        decoder_output = model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask)     \n",
    "        \n",
    "        # Get the last predicted token\n",
    "        output = model.output(decoder_output[:, -1])\n",
    "\n",
    "        # Get the token with the max probability (greedy search)\n",
    "        _, next_word = torch.max(output, dim=1)\n",
    "\n",
    "        # Concatenate the predicted token to the decoder input as the next input for the decoder\n",
    "        decoder_input = torch.cat([decoder_input, torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
    "        \n",
    "        # Break if the decoder predicted the end of sentence token\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    # Remove the batch dimension\n",
    "    decoder_input = decoder_input.squeeze(0)\n",
    "    \n",
    "    # Convert the decoded sentence to a list of token IDs \n",
    "    decoder_input = decoder_input.detach().cpu().numpy()\n",
    "\n",
    "    # Remove the sos token from the decoded sentence\n",
    "    decoder_input = decoder_input[1:]\n",
    "\n",
    "    return decode_tokens(decoder_input, trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, lr, epochs):\n",
    "    # Define the tensorboard writer\n",
    "    writer = SummaryWriter() \n",
    "\n",
    "    # Define the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=1e-8)\n",
    "\n",
    "    # Define the loss function\n",
    "    # Ignore the padding token, which has index 3 in the vocabulary (see function build_vocab in Preprocessing.ipynb file)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=3).to(device)\n",
    "\n",
    "    # Define the checkpoint directory\n",
    "    checkpoint = os.path.join(\"checkpoints\", f\"lr_{lr}\")\n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(checkpoint, exist_ok=True)\n",
    "\n",
    "    # Find the latest checkpoint\n",
    "    latest_epoch = -1\n",
    "    latest_checkpoint_path = None\n",
    "    for fname in os.listdir(checkpoint):\n",
    "        if fname.startswith('epoch_') and fname.endswith('.pth'):\n",
    "            epoch_num = int(fname.split('_')[1].split('.')[0])\n",
    "            if epoch_num > latest_epoch:\n",
    "                latest_epoch = epoch_num\n",
    "                latest_checkpoint_path = os.path.join(checkpoint, fname)\n",
    "\n",
    "    if latest_checkpoint_path:\n",
    "        ckpt = torch.load(latest_checkpoint_path)\n",
    "        model.load_state_dict(ckpt['model_state'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state'])\n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    step = 0 # for logging the loss\n",
    "\n",
    "    for epoch in range (start_epoch, epochs):\n",
    "        # Empty the cache to avoid memory overflow\n",
    "        torch.mps.empty_cache()        \n",
    "        # Set the model to train mode\n",
    "        model.train()\n",
    "\n",
    "        # Create the progress bar \n",
    "        iter = tqdm(train_dl, desc=f'Epoch {epoch}')\n",
    "\n",
    "        # Iterate over the batches\n",
    "        for batch in iter:\n",
    "            # Get the tensors from the batch\n",
    "            encoder_input = batch['encoder_input'].to(device)    # size (batch_size, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device)    # size (batch_size, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)      # size (batch_size, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device)      # size (batch_size, 1, seq_len, seq_len)\n",
    "            label = batch['label'].to(device)                    # size (batch_size, seq_len)\n",
    "\n",
    "            # Run the tensors through the model\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)                                   \n",
    "            decoder_output = model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask)     \n",
    "            output = model.output(decoder_output)                                                        # size (batch_size, seq_len, trg_vocab_size)\n",
    "\n",
    "            # Calculate the loss\n",
    "            # Flatten the output and label tensors to size (batch_size * seq_len, trg_vocab_size)\n",
    "            train_loss = loss_fn(output.view(-1, len(target_vocab)), label.view(-1))\n",
    "            iter.set_postfix(loss=train_loss.item()) # print the loss\n",
    "            writer.add_scalar('Training Loss/Step', train_loss.item(), step) # log the loss\n",
    "            writer.flush() \n",
    "\n",
    "            # Backpropagation\n",
    "            train_loss.backward()    \n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            step += 1\n",
    "        \n",
    "        print(\"Evaluating the model on the validation dataset\")\n",
    "        # Evaluate the model on the validation dataset\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        # Initialize the lists that will contain the references and the outputs for each sentence when computing the BLEU score\n",
    "        references = []\n",
    "        outputs = []\n",
    "\n",
    "        # Disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dl:\n",
    "                # Get the tensors from the batch\n",
    "                encoder_input = batch['encoder_input'].to(device)    \n",
    "                decoder_input = batch['decoder_input'].to(device)    \n",
    "                encoder_mask = batch['encoder_mask'].to(device)      \n",
    "                decoder_mask = batch['decoder_mask'].to(device)      \n",
    "                label = batch['label'].to(device)                    \n",
    "\n",
    "                # Run the tensors through the model\n",
    "                encoder_output = model.encode(encoder_input, encoder_mask)                                   \n",
    "                decoder_output = model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask)     \n",
    "                output = model.output(decoder_output)                                                        # size (batch_size, seq_len, trg_vocab_size)\n",
    "                \n",
    "                # Calculate the loss\n",
    "                # Flatten the output and label tensors to size (batch_size * seq_len, trg_vocab_size)\n",
    "                val_loss += loss_fn(output.view(-1, len(target_vocab)), label.view(-1)).item()\n",
    "\n",
    "                translation = greedy_decode(\n",
    "                    model=model,\n",
    "                    source=encoder_input,\n",
    "                    encoder_mask=encoder_mask,\n",
    "                    trg_vocab = target_vocab,\n",
    "                    sos_idx=1,\n",
    "                    eos_idx=2,\n",
    "                    max_len=header_file['max_seq_len'],\n",
    "                    device=device\n",
    "                )\n",
    "                # Add the generated translation and the reference translation to the lists\n",
    "                outputs.append(translation)\n",
    "                references.append(batch['trg'])  # Assuming this is available in your validation DataLoader\n",
    "\n",
    "        # Log the validation loss\n",
    "        val_loss /= len(val_dl)\n",
    "        writer.add_scalar('Validation Loss/Epoch', val_loss, epoch)\n",
    "        writer.flush()\n",
    "\n",
    "        # Log the BLEU score\n",
    "        bleu_score = corpus_bleu(references, outputs)\n",
    "        writer.add_scalar('BLEU Validation Score/Epoch', bleu_score, epoch)\n",
    "        writer.flush()\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        epoch_checkpoint_path = os.path.join(str(checkpoint), f\"epoch_{epoch}_BLEU_{bleu_score}_train_loss_{round(train_loss.item(), 2)}_val_loss_{round(val_loss, 2)}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state': model.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'bleu_score': bleu_score\n",
    "        }, epoch_checkpoint_path)\n",
    "\n",
    "        print(f\"Checkpoint for epoch {epoch} saved\")\n",
    "        \n",
    "    writer.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1000/1000 [08:47<00:00,  1.90it/s, loss=5.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n",
      "Checkpoint for epoch 0 saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1000/1000 [08:48<00:00,  1.89it/s, loss=4.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the validation dataset\n"
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "    # Define the model\n",
    "    model = build_transformer(\n",
    "                        len(source_vocab),                          # size of the source vocabulary    \n",
    "                        len(target_vocab),                          # size of the target vocabulary\n",
    "                        src_seq_len= header_file[\"max_seq_len\"],    # defined in the header file \n",
    "                        trg_seq_len= header_file[\"max_seq_len\"],    # defined in the header file\n",
    "                        d_model = 512,                              # based on the paper\n",
    "                        N = 3,                                      # number of encoder and decoder layers (we use the same number of layers for both encoder and decoder)\n",
    "                        h = 8,                                      # number of heads (we use the same number of heads for both encoder and decoder)                                          \n",
    "                        dropout = 0.1,                              # based on the paper\n",
    "                        d_ff = 2048                                 # based on the paper\n",
    "                        ).to(device)    \n",
    "    print('Learning rate:', lr)\n",
    "    model = train(model, lr = lr, epochs = epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
