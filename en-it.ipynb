{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from PreProcessing import Preprocessor # for preprocessing the data\n",
    "import json # for reading the config file\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Transformer # for the model\n",
    "from torch.utils.data import DataLoader, TensorDataset # for creating the dataloader\n",
    "# from Transformer import Transformer # for the model\n",
    "from tqdm import tqdm # for the progress bar when training\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter # for tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the header json file were we store paths / parameters / etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON header file\n",
    "def load_json_header(json_file):\n",
    "    with open(json_file) as json_data:\n",
    "        d = json.load(json_data)\n",
    "        return d\n",
    "\n",
    "config = load_json_header('config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the preprocessor\n",
    "preprocessor = Preprocessor()\n",
    "\n",
    "# Load the English-Italian dataset\n",
    "en_file_path = config['en-it-dataset-english'] \n",
    "it_file_path = config['en-it-dataset-italian']\n",
    "\n",
    "en_train, en_val, en_test, it_train, it_val, it_test = preprocessor.preprocess(en_file_path, it_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a securtity check, we print the preprocessed dataset lentghs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Train:  1520698\n",
      "English Validation:  190087\n",
      "English Test:  190088\n",
      "Italian Train:  1520698\n",
      "Italian Validation:  190087\n",
      "Italian Test:  190088\n"
     ]
    }
   ],
   "source": [
    "print(\"English Train: \", len(en_train))\n",
    "print(\"English Validation: \", len(en_val))\n",
    "print(\"English Test: \", len(en_test))\n",
    "print(\"Italian Train: \", len(it_train))\n",
    "print(\"Italian Validation: \", len(it_val))\n",
    "print(\"Italian Test: \", len(it_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming these are the sizes of your vocabularies\n",
    "SRC_VOCAB_SIZE = len(preprocessor.source_tokenizer.word_index) + 1  # +1 for padding token\n",
    "TRG_VOCAB_SIZE = len(preprocessor.target_tokenizer.word_index) + 1\n",
    "\n",
    "# Hyperparameters\n",
    "# Define hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 5\n",
    "src_vocab_size = SRC_VOCAB_SIZE\n",
    "trg_vocab_size = TRG_VOCAB_SIZE\n",
    "embedding_size = 512\n",
    "nhead = 8\n",
    "dim_feedforward = 2048\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "# Set the device mps, cuda or cpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    \n",
    "# Instantiate the Transformer model\n",
    "transformer_model = nn.Transformer(\n",
    "    d_model=embedding_size,\n",
    "    nhead=nhead,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Move model to the device\n",
    "transformer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming these are the sizes of your vocabularies\n",
    "# SRC_VOCAB_SIZE = len(preprocessor.source_tokenizer.word_index) + 1  # +1 for padding token\n",
    "# TRG_VOCAB_SIZE = len(preprocessor.target_tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "# EMB_DIM = 512\n",
    "# N_ENCODER_LAYERS = 3\n",
    "# N_DECODER_LAYERS = 3\n",
    "# ENC_DROPOUT = 0.1\n",
    "# MAX_LENGTH = 100 # Maximum length of the sentence (used for positional encoding)\n",
    "# FORW_EXP = 4 # Forward expansion\n",
    "# SRC_PAD_IDX = 0 # Padding token for the source language\n",
    "# NUM_HEADS = 8 # Number of heads for the multi-head attention\n",
    "\n",
    "# # TensorBoard for logging\n",
    "# writer = SummaryWriter('runs/loss_plot')\n",
    "# step = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(en_train), torch.from_numpy(it_train))\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_data = TensorDataset(torch.from_numpy(en_val), torch.from_numpy(it_val))\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_data = TensorDataset(torch.from_numpy(en_test), torch.from_numpy(it_test))\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.nn import Transformer\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have already created and configured the Transformer model, as well as your optimizer and criterion.\n",
    "\n",
    "def create_src_mask(src, pad_idx):\n",
    "    src_mask = (src != pad_idx).unsqueeze(-2)\n",
    "    return src_mask\n",
    "\n",
    "def create_trg_mask(trg, pad_idx):\n",
    "    trg_pad_mask = (trg != pad_idx).unsqueeze(-2)\n",
    "    trg_len = trg.shape[0]\n",
    "    trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=device)).bool()\n",
    "    trg_mask = trg_pad_mask & trg_sub_mask\n",
    "    return trg_mask\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, pad_idx, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, (src, trg) in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        src_mask = create_src_mask(src, pad_idx)\n",
    "        trg_mask = create_trg_mask(trg, pad_idx)\n",
    "\n",
    "        output = model(src, trg[:-1], src_mask=src_mask, tgt_mask=trg_mask)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Hyperparameters\n",
    "PAD_IDX = 0  # Update this to your dataset's padding index\n",
    "CLIP = 1\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    train_loss = train(transformer_model, train_loader, optimizer, criterion, PAD_IDX, CLIP)\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enzobenoit-jeannin/Documents/GitHub/COMP550/myenv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# model = Transformer(\n",
    "#     src_vocab_size=SRC_VOCAB_SIZE,\n",
    "#     trg_vocab_size=TRG_VOCAB_SIZE,\n",
    "#     src_pad_idx=SRC_PAD_IDX,\n",
    "#     embedding_size=EMB_DIM,\n",
    "#     num_encoder_layers=N_ENCODER_LAYERS,\n",
    "#     num_decoder_layers=N_DECODER_LAYERS,\n",
    "#     forward_expansion=FORW_EXP,\n",
    "#     max_len=MAX_LENGTH,\n",
    "#     dropout=ENC_DROPOUT,\n",
    "#     num_heads=NUM_HEADS,\n",
    "#     device=device\n",
    "# ).to(device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index = SRC_PAD_IDX)\n",
    "\n",
    "# save_model = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
